{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# plotting options\n",
    "%matplotlib inline\n",
    "np.set_printoptions(linewidth=250)\n",
    "plt.rc('font'  , size=18)\n",
    "plt.rc('figure', figsize=(10, 8))\n",
    "plt.rc('axes'  , labelsize=22)\n",
    "plt.rc('legend', fontsize=16)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "plt.rc('figure', figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera/Development/dst/courses/coursera_bigdata/course3/week5\n"
     ]
    }
   ],
   "source": [
    "os.chdir('%s/courses/coursera_bigdata/course3/week5' % os.getenv('DST'))\n",
    "pwd = os.getcwd()\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "spark_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, SQLContext, HiveContext\n",
    "\n",
    "myConf = SparkConf().setAppName('TestApp')\\\n",
    "                    .set('spark.executor.memory', '2G')\\\n",
    "                    .set('spark.hadoop.validateOutputSpecs', 'false')\n",
    "\n",
    "sc      = SparkContext(conf=myConf)\n",
    "sql_ctx = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_ctx.createDataFrame([(\"somekey\", 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Slides walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A long time ago in a galaxy far far away']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_RDD = sc.textFile('file:%s/testfile1.txt'%pwd)\n",
    "text_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', 1),\n",
       " (u'long', 1),\n",
       " (u'time', 1),\n",
       " (u'ago', 1),\n",
       " (u'in', 1),\n",
       " (u'a', 1),\n",
       " (u'galaxy', 1),\n",
       " (u'far', 1),\n",
       " (u'far', 1),\n",
       " (u'away', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_words(line):\n",
    "    return line.split()\n",
    "\n",
    "def create_pair(word):\n",
    "    return (word, 1)\n",
    "\n",
    "pairs_RDD = text_RDD.flatMap(split_words).map(create_pair)\n",
    "pairs_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100, 'Ryan', 8.5, 'computer science'],\n",
       " [101, 'Bob', 7.1, 'engineering'],\n",
       " [101, 'Carl', 6.2, 'engineering']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students = sc.parallelize([\n",
    "    [100, 'Ryan', 8.5, 'computer science'],\n",
    "    [101, 'Bob' , 7.1, 'engineering'     ],\n",
    "    [101, 'Carl', 6.2, 'engineering'     ]\n",
    "])\n",
    "students.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.266666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_grade(row):\n",
    "    return row[2]\n",
    "\n",
    "students.map(extract_grade).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computer science', 8.5), ('engineering', 7.1), ('engineering', 6.2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_degree_grade(row):\n",
    "    return (row[3], row[2])\n",
    "\n",
    "students.map(extract_degree_grade).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('engineering', 7.1), ('computer science', 8.5)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_grade_RDD = students.map(extract_degree_grade)\n",
    "degree_grade_RDD.reduceByKey(max).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- grade: double (nullable = true)\n",
      " |-- degree: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_df = sql_ctx.createDataFrame(students, ['id', 'name', 'grade', 'degree'])\n",
    "students_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AVG(grade#4)=7.266666666666667)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_df.agg({'grade': 'mean'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(degree=u'engineering', MAX(grade#4)=7.1),\n",
       " Row(degree=u'computer science', MAX(grade#4)=8.5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_df.groupBy('degree').max('grade').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree           MAX(grade#4)\n",
      "engineering      7.1         \n",
      "computer science 8.5         \n"
     ]
    }
   ],
   "source": [
    "students_df.groupBy('degree').max('grade').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,LongType,true),StructField(name,StringType,true),StructField(grade,DoubleType,true),StructField(degree,StringType,true)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id'    , LongType()  , True),\n",
    "    StructField('name'  , StringType(), True),\n",
    "    StructField('grade' , DoubleType(), True),\n",
    "    StructField('degree', StringType(), True),\n",
    "])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id  name grade degree          \n",
      "100 Ryan 8.5   computer science\n",
      "101 Bob  7.1   engineering     \n",
      "101 Carl 6.2   engineering     \n"
     ]
    }
   ],
   "source": [
    "students_df = sql_ctx.createDataFrame(students, schema)\n",
    "students_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\":100, \"name\":\"Alice\", \"grade\":8.5, \"degree\":\"Computer Science\"}\\n{\"id\":101, \"name\":\"Bob\", \"grade\":7.1, \"degree\":\"Engineering\"}\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_json = \"\"\"\\\n",
    "{\"id\":100, \"name\":\"Alice\", \"grade\":8.5, \"degree\":\"Computer Science\"}\n",
    "{\"id\":101, \"name\":\"Bob\", \"grade\":7.1, \"degree\":\"Engineering\"}\n",
    "\"\"\"\n",
    "students_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "with open(\"students.json\", 'w+') as f:\n",
    "    f.write(students_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree           grade id  name \n",
      "Computer Science 8.5   100 Alice\n",
      "Engineering      7.1   101 Bob  \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.jsonFile('file:%s/students.json'%pwd).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_df = sql_ctx.load(\n",
    "    source      ='com.databricks.spark.csv',\n",
    "    header      = 'true',\n",
    "    inferSchema = 'true',\n",
    "    path        = 'file:%s/index_data.csv'%pwd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_address: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- neighborhoods: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000L"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<useful>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<useful>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df['useful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[useful: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.select('useful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601L"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.filter(yelp_df.useful >= 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601L"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.filter(yelp_df['useful'] >= 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601L"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.filter('useful >= 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'exceptions.AttributeError'> 'Column' object has no attribute 'agg'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    yelp_df['useful'].agg({'useful':'max'}).collect()\n",
    "except Exception, e:\n",
    "    print type(e), e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[useful: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.select('useful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(MAX(useful#45)=28)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.select('useful').agg({'useful': 'max'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'fWKvX83p0-ka4JS3dc6E5A', useful=5),\n",
       " Row(id=u'IjZ33sJrzXqU-0X6U8NwyA', useful=0),\n",
       " Row(id=u'IESLBzqUCLdSzSqm0eCSxQ', useful=1),\n",
       " Row(id=u'G-WvGaISbqqaMHlNnByodA', useful=2),\n",
       " Row(id=u'1uJFq2r5QfJG_6ExMRCaGw', useful=0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.select('id', 'useful').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   ((useful / 28) * 100)\n",
      "fWKvX83p0-ka4JS3d... 17.857142857142858   \n",
      "IjZ33sJrzXqU-0X6U... 0.0                  \n",
      "IESLBzqUCLdSzSqm0... 3.571428571428571    \n",
      "G-WvGaISbqqaMHlNn... 7.142857142857142    \n",
      "1uJFq2r5QfJG_6ExM... 0.0                  \n"
     ]
    }
   ],
   "source": [
    "yelp_df.select('id', yelp_df.useful/28*100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   CAST(((useful / 28) * 100), IntegerType)\n",
      "fWKvX83p0-ka4JS3d... 17                                      \n",
      "IjZ33sJrzXqU-0X6U... 0                                       \n",
      "IESLBzqUCLdSzSqm0... 3                                       \n",
      "G-WvGaISbqqaMHlNn... 7                                       \n",
      "1uJFq2r5QfJG_6ExM... 0                                       \n"
     ]
    }
   ],
   "source": [
    "yelp_df.select('id', (yelp_df.useful/28*100).cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   useful_scaled\n",
      "fWKvX83p0-ka4JS3d... 17           \n",
      "IjZ33sJrzXqU-0X6U... 0            \n",
      "IESLBzqUCLdSzSqm0... 3            \n",
      "G-WvGaISbqqaMHlNn... 7            \n",
      "1uJFq2r5QfJG_6ExM... 0            \n"
     ]
    }
   ],
   "source": [
    "yelp_df.select('id', (yelp_df.useful/28*100).cast('int').alias('useful_scaled')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- useful_scaled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "useful_perc_data = yelp_df.select(\n",
    "    yelp_df['id'].alias('uid'), \n",
    "    (yelp_df.useful/28*100).cast('int').alias('useful_scaled')\n",
    ")\n",
    "useful_perc_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ordering by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- useful_scaled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "useful_perc_data = yelp_df.select(\n",
    "    yelp_df['id'].alias('uid'), \n",
    "    (yelp_df.useful/28*100).cast('int').alias('useful_scaled')\n",
    ").orderBy(desc('useful_scaled'))\n",
    "\n",
    "useful_perc_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                  useful_scaled\n",
      "RqwFPp_qPu-1h87pG... 100          \n",
      "YAXPKM-Hck6-mjF74... 82           \n",
      "WRBYytJAaJI1BTQG5... 71           \n",
      "sA_wkvAZpt4Hm6AXG... 71           \n",
      "roMeHsyf55-_O7rpu... 67           \n"
     ]
    }
   ],
   "source": [
    "useful_perc_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_df = useful_perc_data.join(\n",
    "    yelp_df,\n",
    "    yelp_df.id==useful_perc_data.uid,\n",
    "    'inner'\n",
    ").select(useful_perc_data.uid, 'useful_scaled', 'review_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- useful_scaled: integer (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                  useful_scaled review_count\n",
      "WRBYytJAaJI1BTQG5... 71            362         \n",
      "GXj4PNAi095-q9ynP... 3             76          \n",
      "1sn0-eY_d1Dhr6Q2u... 0             9           \n",
      "MtFe-FuiOmo0vlo16... 0             7           \n",
      "EMYmuTlyeNBy5QB9P... 7             19          \n"
     ]
    }
   ],
   "source": [
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                  useful_scaled review_count\n",
      "WRBYytJAaJI1BTQG5... 71            362         \n",
      "GXj4PNAi095-q9ynP... 3             76          \n",
      "1sn0-eY_d1Dhr6Q2u... 0             9           \n",
      "MtFe-FuiOmo0vlo16... 0             7           \n",
      "EMYmuTlyeNBy5QB9P... 7             19          \n"
     ]
    }
   ],
   "source": [
    "joined_df = useful_perc_data.join(\n",
    "    yelp_df,\n",
    "    yelp_df.id==useful_perc_data.uid,\n",
    "    'inner'\n",
    ").cache().select(useful_perc_data.uid, 'useful_scaled', 'review_count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                  useful_scaled review_count\n",
      "WRBYytJAaJI1BTQG5... 71            362         \n",
      "GXj4PNAi095-q9ynP... 3             76          \n",
      "1sn0-eY_d1Dhr6Q2u... 0             9           \n",
      "MtFe-FuiOmo0vlo16... 0             7           \n",
      "EMYmuTlyeNBy5QB9P... 7             19          \n"
     ]
    }
   ],
   "source": [
    "joined_df = useful_perc_data.join(\n",
    "    yelp_df,\n",
    "    yelp_df.id==useful_perc_data.uid,\n",
    "    'inner'\n",
    ").cache().select(\n",
    "    useful_perc_data.uid, \n",
    "    'useful_scaled', \n",
    "    'review_count'\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### server logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_ctx._jsc.hadoopConfiguration().set('textinputformat.record.delimiter', '\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      " |-- app: string (nullable = true)\n",
      " |-- user_agent_major: integer (nullable = true)\n",
      " |-- region_code: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- subapp: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- client_ip: string (nullable = true)\n",
      " |-- user_agent_family: string (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      " |-- referer: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- extension: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- os_major: integer (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- device_family: string (nullable = true)\n",
      " |-- record: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- os_family: string (nullable = true)\n",
      " |-- country_code3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df = sql_ctx.load(\n",
    "    source      = 'com.databricks.spark.csv',\n",
    "    header      = 'true',\n",
    "    inferSchema = 'true',\n",
    "    path        = 'file:%s/logs.csv'%pwd\n",
    ")\n",
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9410L"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code protocol request              app       user_agent_major region_code country_code id                   city      subapp latitude           method client_ip       user_agent_family bytes referer country_name extension url                  os_major longitude          device_family record               user_agent           time                 os_family country_code3\n",
      "200  HTTP/1.1 GET /metastore/ta... metastore null             00          SG           8836e6ce-9a21-449... Singapore table  1.2931000000000097 GET    128.199.234.236 Other             1041  -       Singapore              /metastore/table/... null     103.85579999999999 Other         demo.gethue.com:8... Mozilla/5.0 (comp... 2014-05-04T06:35:49Z Other     SGP          \n",
      "200  HTTP/1.1 GET /metastore/ta... metastore null             00          SG           6ddf6e38-7b83-423... Singapore table  1.2931000000000097 GET    128.199.234.236 Other             1041  -       Singapore              /metastore/table/... null     103.85579999999999 Other         demo.gethue.com:8... Mozilla/5.0 (comp... 2014-05-04T06:35:50Z Other     SGP          \n",
      "200  HTTP/1.1 GET /search/?coll... search    null             00          SG           313bb28e-dd7c-436... Singapore        1.2931000000000097 GET    128.199.234.236 Other             1041  -       Singapore              /search/?collecti... null     103.85579999999999 Other         demo.gethue.com:8... Mozilla/5.0 (comp... 2014-05-04T06:35:52Z Other     SGP          \n",
      "200  HTTP/1.1 GET /search/?coll... search    null             00          SG           ecb47c61-a9e4-4b5... Singapore        1.2931000000000097 GET    128.199.234.236 Other             1041  -       Singapore              /search/?collecti... null     103.85579999999999 Other         demo.gethue.com:8... Mozilla/5.0 (comp... 2014-05-04T06:35:53Z Other     SGP          \n",
      "200  HTTP/1.1 HEAD / HTTP/1.1                null             00          SG           affdb6b9-3657-4d1... Singapore        1.2931000000000097 HEAD   128.199.234.236 Other             238   -       Singapore              /                    null     103.85579999999999 Other         demo.gethue.com:8... Mozilla/5.0 (comp... 2014-05-04T06:35:54Z Other     SGP          \n"
     ]
    }
   ],
   "source": [
    "logs_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code count\n",
      "500  2    \n",
      "301  71   \n",
      "302  1943 \n",
      "502  6    \n",
      "304  117  \n",
      "400  1    \n",
      "200  7235 \n",
      "401  10   \n",
      "404  11   \n",
      "408  14   \n"
     ]
    }
   ],
   "source": [
    "logs_df.groupBy('code').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code count\n",
      "200  7235 \n",
      "302  1943 \n",
      "304  117  \n",
      "301  71   \n",
      "408  14   \n",
      "404  11   \n",
      "401  10   \n",
      "502  6    \n",
      "500  2    \n",
      "400  1    \n"
     ]
    }
   ],
   "source": [
    "logs_df.groupBy('code').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code AVG(bytes#406)    \n",
      "500  4684.5            \n",
      "301  424.61971830985914\n",
      "302  415.6510550694802 \n",
      "502  581.0             \n",
      "304  185.26495726495727\n",
      "400  0.0               \n",
      "200  41750.03759502419 \n",
      "401  12472.8           \n",
      "404  17872.454545454544\n",
      "408  440.57142857142856\n"
     ]
    }
   ],
   "source": [
    "logs_df.groupBy('code').avg('bytes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code AVG(bytes#406)     MIN(bytes#406) MAX(bytes#406)\n",
      "500  4684.5             422            8947          \n",
      "301  424.61971830985914 331            499           \n",
      "302  415.6510550694802  304            1034          \n",
      "502  581.0              581            581           \n",
      "304  185.26495726495727 157            204           \n",
      "400  0.0                0              0             \n",
      "200  41750.03759502419  0              9045352       \n",
      "401  12472.8            8318           28895         \n",
      "404  17872.454545454544 7197           23822         \n",
      "408  440.57142857142856 0              514           \n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "logs_df.groupBy('code').agg(\n",
    "    logs_df.code,\n",
    "    F.avg(logs_df.bytes),\n",
    "    F.min(logs_df.bytes),\n",
    "    F.max(logs_df.bytes)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_ctx._jsc.hadoopConfiguration().set('textinputformat.record.delimiter', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_df = sql_ctx.load(\n",
    "    source      ='com.databricks.spark.csv',\n",
    "    header      = 'true',\n",
    "    inferSchema = 'true',\n",
    "    path        = 'file:%s/index_data.csv'%pwd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_address: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- neighborhoods: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id          cool date       funny id                   stars text                 type     useful user_id              name               full_address         latitude      longitude      neighborhoods open review_count state\n",
      "9yKzy9PApeiPPOUJE... 2    2011-01-26 0     fWKvX83p0-ka4JS3d... 4     My wife took me h... business 5      rLtl8ZkDX5vH5nAx9... Morning Glory Cafe 6106 S 32nd St Ph... 33.3907928467 -112.012504578 []            True 116          AZ   \n",
      "ZRJwVLyzEJq1VAihD... 0    2011-07-27 0     IjZ33sJrzXqU-0X6U... 4     I have no idea wh... business 0      0a2KyEL0d3Yb1V6ai... Spinato's Pizzeria 4848 E Chandler B... 33.305606842  -111.978759766 []            True 102          AZ   \n",
      "6oRAC4uyJCsJl1X0W... 0    2012-06-14 0     IESLBzqUCLdSzSqm0... 4     love the gyro pla... business 1      0hT2KtfLiobPvh6cD... Haji-Baba          1513 E  Apache Bl... 33.4143447876 -111.913032532 []            True 265          AZ   \n",
      "_1QQZuf4zZOyFCvXc... 1    2010-05-27 0     G-WvGaISbqqaMHlNn... 4     Rosie, Dakota, an... business 2      uZetl9T0NcROGOyFf... Chaparral Dog Park 5401 N Hayden Rd ... 33.5229454041 -111.90788269  []            True 88           AZ   \n",
      "6ozycU1RpktNG2-1B... 0    2012-01-05 0     1uJFq2r5QfJG_6ExM... 4     General Manager S... business 0      vYmM4KTsC8ZfQBg-j... Discount Tire      1357 S Power Road... 33.3910255432 -111.68447876  []            True 5            AZ   \n"
     ]
    }
   ],
   "source": [
    "yelp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG(cool#472)\n",
      "0.998        \n"
     ]
    }
   ],
   "source": [
    "yelp_df.select('cool').agg({'cool':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stars AVG(cool#472)     \n",
      "2     0.5217391304347826\n",
      "3     1.0817610062893082\n",
      "4     1.0675944333996024\n",
      "5     2.2222222222222223\n"
     ]
    }
   ],
   "source": [
    "yelp_df.filter('review_count>=10').groupBy('stars').mean('cool').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG(cool#472)\n",
      "2.25         \n"
     ]
    }
   ],
   "source": [
    "yelp_df.filter('review_count>=10 and open=\"True\" and stars=5').agg({'cool':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stars AVG(cool#472)     \n",
      "2     0.6               \n",
      "3     1.0456140350877192\n",
      "4     1.0759219088937093\n",
      "5     2.2857142857142856\n"
     ]
    }
   ],
   "source": [
    "yelp_df.filter('review_count>10 and open=\"True\"').groupBy('stars').mean('cool').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o455.sort.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'sum(review_count)' given input columns state, SUM(review_count#487);\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:249)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:263)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:103)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:117)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:116)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3.apply(CheckAnalysis.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3.apply(CheckAnalysis.scala:43)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.apply(CheckAnalysis.scala:43)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1069)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame.logicalPlanToDataFrame(DataFrame.scala:157)\n\tat org.apache.spark.sql.DataFrame.sort(DataFrame.scala:403)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-1a91748b015b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myelp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'review_count>=10 and open=\"True\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'review_count'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sum(review_count)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msort\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m    524\u001b[0m         jcols = ListConverter().convert([_to_java_column(c) for c in cols],\n\u001b[0;32m    525\u001b[0m                                         self._sc._gateway._gateway_client)\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o455.sort.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'sum(review_count)' given input columns state, SUM(review_count#487);\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:249)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:263)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:247)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:103)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:117)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:116)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3.apply(CheckAnalysis.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3.apply(CheckAnalysis.scala:43)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.apply(CheckAnalysis.scala:43)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1069)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame.logicalPlanToDataFrame(DataFrame.scala:157)\n\tat org.apache.spark.sql.DataFrame.sort(DataFrame.scala:403)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "yelp_df.filter('review_count>=10 and open=\"True\"')\\\n",
    ".groupBy('state')\\\n",
    ".sum('review_count')\\\n",
    ".orderBy(desc('sum(review_count)'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_count\n",
      "72214           \n",
      "8394            \n",
      "6720            \n",
      "5764            \n",
      "4081            \n",
      "3470            \n",
      "2125            \n",
      "1876            \n",
      "1778            \n",
      "525             \n",
      "429             \n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "yelp_df.filter('review_count>=10 and open=\"True\"')\\\n",
    ".groupBy('state')\\\n",
    ".agg(F.sum('review_count').alias('max_review_count'))\\\n",
    ".orderBy(desc('max_review_count'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venue_count\n",
      "6          \n",
      "6          \n",
      "4          \n",
      "4          \n",
      "4          \n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "yelp_df.groupBy('business_id').agg(\n",
    "    F.count(yelp_df.business_id).alias('venue_count'),\n",
    ").orderBy(desc('venue_count')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- full_address: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- neighborhoods: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_df = sql_ctx.load(\n",
    "    source      ='com.databricks.spark.csv',\n",
    "    header      = 'true',\n",
    "    inferSchema = 'true',\n",
    "    path        = 'file:%s/index_data.csv'%pwd\n",
    ")\n",
    "yelp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_df.registerTempTable('yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, cool: int, date: string, funny: int, id: string, stars: int, text: string, type: string, useful: int, user_id: string, name: string, full_address: string, latitude: double, longitude: double, neighborhoods: string, open: string, review_count: int, state: string]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_yelp = sql_ctx.sql(\"\"\"\\\n",
    "select * \n",
    "from yelp\n",
    "where useful >= 1\n",
    "\"\"\")\n",
    "filtered_yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601L, 601L)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.filter(yelp_df.useful>=1).count(), filtered_yelp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[max_useful: int]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\\\n",
    "select \n",
    "    max(useful) as max_useful\n",
    "from yelp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX(useful#568)\n",
      "28             \n"
     ]
    }
   ],
   "source": [
    "yelp_df.agg({'useful':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, useful_scaled: int, review_count: int]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_perc_data.join(\n",
    "    yelp_df,\n",
    "    yelp_df.id==useful_perc_data.uid,\n",
    "    'inner'    \n",
    ").select(\n",
    "    useful_perc_data.uid,\n",
    "    'useful_scaled',\n",
    "    'review_count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_perc_data.registerTempTable('useful_perc_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- useful_scaled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "useful_perc_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, useful_scaled: int, review_count: int]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\\\n",
    "select\n",
    "    useful_perc_data.uid\n",
    "  , useful_scaled\n",
    "  , review_count\n",
    "from useful_perc_data\n",
    "    inner join yelp on useful_perc_data.uid=yelp.id\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### HIVE tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id customer_fname customer_lname customer_email customer_password customer_street      customer_city customer_state customer_zipcode\n",
      "1           Richard        Hernandez      XXXXXXXXX      XXXXXXXXX         6303 Heather Plaza   Brownsville   TX             78521           \n",
      "2           Mary           Barrett        XXXXXXXXX      XXXXXXXXX         9526 Noble Embers... Littleton     CO             80126           \n",
      "3           Ann            Smith          XXXXXXXXX      XXXXXXXXX         3422 Blue Pioneer... Caguas        PR             00725           \n",
      "4           Mary           Jones          XXXXXXXXX      XXXXXXXXX         8324 Little Common   San Marcos    CA             92069           \n",
      "5           Robert         Hudson         XXXXXXXXX      XXXXXXXXX         10 Crystal River ... Caguas        PR             00725           \n"
     ]
    }
   ],
   "source": [
    "customers_df = sql_ctx.sql('select * from customers')\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- customer_lname: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_password: string (nullable = true)\n",
      " |-- customer_street: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_zipcode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_name        item_count\n",
      "Cleats               24551     \n",
      "Men's Footwear       22246     \n",
      "Women's Apparel      21035     \n",
      "Indoor/Outdoor Games 19298     \n",
      "Fishing              17325     \n",
      "Water Sports         15540     \n",
      "Camping & Hiking     13729     \n",
      "Cardio Equipment     12487     \n",
      "Shop By Sport        10984     \n",
      "Electronics          3156      \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\\\n",
    "select \n",
    "    c.category_name\n",
    "  , count(order_item_quantity) as item_count\n",
    "from order_items oi\n",
    "  inner join products p on oi.order_item_product_id=p.product_id\n",
    "  inner join categories c on c.category_id=p.product_category_id\n",
    "group by c.category_name\n",
    "order by item_count desc\n",
    "limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id product_name         revenue           \n",
      "1004       Field & Stream Sp... 6929653.690338135 \n",
      "365        Perfect Fitness P... 4421143.14352417  \n",
      "957        Diamondback Women... 4118425.570831299 \n",
      "191        Nike Men's Free 5... 3667633.196662903 \n",
      "502        Nike Men's Dri-FI... 3147800.0         \n",
      "1073       Pelican Sunstream... 3099845.085144043 \n",
      "403        Nike Men's CJ Eli... 2891757.6622009277\n",
      "1014       O'Brien Men's Neo... 2888993.91355896  \n",
      "627        Under Armour Girl... 1269082.6712722778\n",
      "565        adidas Youth Germ... 67830.0           \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\\\n",
    "select\n",
    "    p.product_id   as product_id\n",
    "  , p.product_name as product_name\n",
    "  , r.revenue      as revenue\n",
    "from products p\n",
    "  inner join (\n",
    "    select \n",
    "        oi.order_item_product_id as product_id\n",
    "      , sum(cast(oi.order_item_subtotal as float)) as revenue\n",
    "   from order_items oi\n",
    "      inner join orders o on oi.order_item_order_id=o.order_id\n",
    "    group by order_item_product_id\n",
    "  ) r on p.product_id=r.product_id\n",
    "order by r.revenue desc\n",
    "limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_df.saveAsTable('yelp_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id          cool date       funny id                   stars text                 type     useful user_id              name               full_address         latitude      longitude      neighborhoods open review_count state\n",
      "9yKzy9PApeiPPOUJE... 2    2011-01-26 0     fWKvX83p0-ka4JS3d... 4     My wife took me h... business 5      rLtl8ZkDX5vH5nAx9... Morning Glory Cafe 6106 S 32nd St Ph... 33.3907928467 -112.012504578 []            True 116          AZ   \n",
      "ZRJwVLyzEJq1VAihD... 0    2011-07-27 0     IjZ33sJrzXqU-0X6U... 4     I have no idea wh... business 0      0a2KyEL0d3Yb1V6ai... Spinato's Pizzeria 4848 E Chandler B... 33.305606842  -111.978759766 []            True 102          AZ   \n",
      "6oRAC4uyJCsJl1X0W... 0    2012-06-14 0     IESLBzqUCLdSzSqm0... 4     love the gyro pla... business 1      0hT2KtfLiobPvh6cD... Haji-Baba          1513 E  Apache Bl... 33.4143447876 -111.913032532 []            True 265          AZ   \n",
      "_1QQZuf4zZOyFCvXc... 1    2010-05-27 0     G-WvGaISbqqaMHlNn... 4     Rosie, Dakota, an... business 2      uZetl9T0NcROGOyFf... Chaparral Dog Park 5401 N Hayden Rd ... 33.5229454041 -111.90788269  []            True 88           AZ   \n",
      "6ozycU1RpktNG2-1B... 0    2012-01-05 0     1uJFq2r5QfJG_6ExM... 4     General Manager S... business 0      vYmM4KTsC8ZfQBg-j... Discount Tire      1357 S Power Road... 33.3910255432 -111.68447876  []            True 5            AZ   \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql('select * from yelp_reviews').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "1558 \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select\n",
    "    sum(1) as count\n",
    "from orders o\n",
    "where o.order_status='SUSPECTED_FRAUD'\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- order_item_quantity: integer (nullable = true)\n",
      " |-- order_item_subtotal: float (nullable = true)\n",
      " |-- order_item_product_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select * from order_items oi\n",
    "\"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_amount      \n",
      "3449.9099884033203\n",
      "2859.8900032043457\n",
      "2839.9100036621094\n",
      "2779.8600006103516\n",
      "2699.899990081787 \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select\n",
    "    sum(cast(order_item_subtotal as float)) as total_amount\n",
    "from order_items oi\n",
    "group by order_item_order_id\n",
    "order by total_amount desc\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- order_item_quantity: integer (nullable = true)\n",
      " |-- order_item_subtotal: float (nullable = true)\n",
      " |-- order_item_product_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select * from order_items oi\n",
    "\"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: long (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select * from orders o\n",
    "\"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_price         \n",
      "133.18070529114834\n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select\n",
    "    avg(oi.order_item_product_price) as avg_price\n",
    "from order_items oi\n",
    "  inner join orders o on oi.order_item_order_id=o.order_id\n",
    "where 1=1\n",
    "  and o.order_status='COMPLETE'\n",
    "\"\"\").cache().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_id  total_amount      \n",
      "9337  6585.330139160156 \n",
      "3710  6169.4001388549805\n",
      "10744 5799.500072479248 \n",
      "749   5759.540145874023 \n",
      "5411  5174.560092926025 \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select\n",
    "    o.order_customer_id as c_id\n",
    "  , sum(oi.order_item_subtotal) as total_amount\n",
    "from order_items oi\n",
    "  inner join orders o on oi.order_item_order_id=o.order_id\n",
    "where 1=1\n",
    "  and o.order_status='COMPLETE'\n",
    "group by o.order_customer_id\n",
    "order by total_amount desc\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id total_amount      \n",
      "68809    2779.8600006103516\n",
      "68766    2699.899990081787 \n",
      "68848    2399.959991455078 \n",
      "68875    2399.949981689453 \n",
      "68816    2329.939987182617 \n"
     ]
    }
   ],
   "source": [
    "sql_ctx.sql(\"\"\"\n",
    "select\n",
    "    o.order_id as order_id\n",
    "  , sum(oi.order_item_subtotal) as total_amount\n",
    "from order_items oi\n",
    "  inner join orders o on oi.order_item_order_id=o.order_id\n",
    "where 1=1\n",
    "  and o.order_status!='COMPLETE'\n",
    "group by o.order_id\n",
    "order by total_amount desc\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
