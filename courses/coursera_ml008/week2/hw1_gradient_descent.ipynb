{
 "metadata": {
  "name": "",
  "signature": "sha256:c2e83a592b7aa82f6edda673af3a6a3243ded885f7a37242ce82953d429191cd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gradient Descent and Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First consider the 1-D case.  Assume that $x$ is our dependent variable (feature) and $y$ is the variable we wish to predict (predictor).  Given a set of $m$ observations of ($x$, $y$) training values, we wish to find a model that predicts the data.  The linear model uses parameters $\\theta_0$ and $\\theta_1$ to construct a linear hypothesis:\n",
      "\n",
      "\\begin{align}\n",
      "h_{\\theta}(x) &= \\theta_0 + \\theta_1 x_1 \\\\\n",
      "&= \\theta_0 x_0 + \\theta_1 x_1 \\\\\n",
      "&= \\boldsymbol{\\theta}^T\\ {\\bf x} \\\\\n",
      "\\end{align}\n",
      "\n",
      "In order to find an optimum model, we minimize the \"cost function\", $J(\\theta)$, w.r.t. $\\theta$ where\n",
      "\n",
      "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2 $$.\n",
      "\n",
      "\n",
      "The goal of regression is to find an optimum fit for a dataset using a linear model.  Here we are summing over the \"training set\" of $m$ observations.\n",
      "\n",
      "Gradient descent is a numerical method to find that minimum if $J(\\theta)$ by incrementally moving towards the the minimum of the $\\theta$ parameter space.  An assumpution of this method is that the model is convex in parameter space.  This is done by walking to the minimum by taking steps defined by:\n",
      "\n",
      "\\begin{align}\n",
      "{\\bf \\theta} &\\leftarrow {\\bf \\theta} - \\alpha \\nabla J(\\theta) \\\\\n",
      "\\end{align}\n",
      "In index notation, \n",
      "\\begin{align}\n",
      "\\theta_f &\\leftarrow \\theta_f - \\alpha \\sum_{f=1}^{N_f} \\frac{\\partial}{\\partial \\theta_f} J(\\theta)\n",
      "\\end{align}\n",
      "where\n",
      "\\begin{align}\n",
      "\\frac{\\partial}{\\partial \\theta_f} J(\\theta) &=\n",
      "\\frac{1}{m} \\sum_{i=1}^{m} \\left(  \\sum_f \\theta_f x_f^{(i)} - y^{(i)} \\right) \\sum_{f'} \\frac{\\partial \\theta_{f'}}{\\partial \\theta_f} x_{f'}^{(i)} \\\\\n",
      "&= \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\sum_f \\theta_f x_f^{(i)} - y^{(i)} \\right) \\sum_{f'} \\delta_{ff'} x_{f'}^{(i)} \\\\\n",
      "&= \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\sum_f \\theta_f x_f^{(i)} - y^{(i)} \\right) x_{f}^{(i)} \\\\\n",
      "&= \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\boldsymbol{\\theta} \\cdot {\\bf x}^{(i)} - y^{(i)} \\right) x_{f}^{(i)} \\\\\n",
      "\\end{align}\n",
      "\n",
      "where ${\\bf x}^{(i)} = (x_0^{(i)}, x_1^{(i)})$ for the $i^{th}$ datum.  \n",
      "\n",
      "## Implementation details\n",
      "\n",
      "When computing gradient descent in a program such as MATLAB or python, it is customary to represent the data as a Matrix.  The convention that we pick is the training dimension $(i)$ will be denoted by rows and the feature dimension $(j)$ will be determined by columns.  This gives a matrix of the form:\n",
      "\n",
      "\\begin{align}\n",
      "{\\bf X} = \n",
      "\\left( \n",
      "\\begin{array}{ccc}\n",
      "1 & x_1^{(1)}\\\\\n",
      "1 & x_1^{(2)}\\\\\n",
      "\\vdots & \\vdots \\\\\n",
      "1 & x_1^{(m)}\\\\\n",
      "\\end{array} \n",
      "\\right)\n",
      "\\end{align}\n",
      "\n",
      "For convenience, we denote by $x_0^{(i)} = 1$:\n",
      "\n",
      "\\begin{align}\n",
      "{\\bf X} = \n",
      "\\left( \n",
      "\\begin{array}{ccc}\n",
      "x_0^{(1)} & x_1^{(1)}\\\\\n",
      "x_0^{(2)} & x_1^{(2)}\\\\\n",
      "\\vdots & \\vdots \\\\\n",
      "x_0^{(m)} & x_1^{(m)}\\\\\n",
      "\\end{array} \n",
      "\\right)\n",
      "\\end{align}\n",
      "\n",
      "Or you can think of each row as vector representing the $i^{th}$ data point.\n",
      "\n",
      "\\begin{align}\n",
      "{\\bf X} = \n",
      "\\left( \n",
      "\\begin{array}{c}\n",
      "-\\ {{\\bf x}^{(1)} }^T - \\\\\n",
      "-\\ {{\\bf x}^{(2)} }^T - \\\\\n",
      "\\vdots  \\\\\n",
      "-\\ {{\\bf x}^{(m)} }^T - \\\\\n",
      "\\end{array} \n",
      "\\right)\n",
      "\\end{align}\n",
      "\n",
      "In vectorized notation (.i.e. MATLAB or numpy), we must transpose the ${\\bf X}$ matrix so that the columns are now vectors in feature space and the rows are vectors in training dataset space.  \n",
      "In short, $h_{\\theta}(\n",
      "\n",
      "\\begin{align}\n",
      "h_{\\theta}({\\bf X}) \n",
      "&= \\left( \n",
      "\\begin{array}{c}\n",
      "\\theta_0 x_0^{(1)} + \\dots +  \\theta_n x_n^{(1)} \\\\\n",
      "\\theta_0 x_0^{(2)} + \\dots +  \\theta_n x_n^{(2)} \\\\\n",
      "\\vdots \\\\\n",
      "\\theta_0 x_0^{(m)} + \\dots +  \\theta_n x_n^{(m)} \\\\\n",
      "\\end{array}\n",
      "\\right) \\\\\n",
      "&= \\left( \n",
      "\\begin{array}{ccc}\n",
      "x_0^{(1)} & \\dots  & x_n^{(1)} \\\\\n",
      "x_0^{(2)} & \\dots  & x_n^{(2)} \\\\\n",
      "          & \\vdots &           \\\\\n",
      "x_0^{(m)} & \\dots  & x_n^{(m)} \\\\\n",
      "\\end{array}\n",
      "\\right)\n",
      "\\left( \n",
      "\\begin{array}{c}\n",
      "\\theta_0\\\\\n",
      "\\theta_1\\\\\n",
      "\\vdots \\\\\n",
      "\\theta_n\\\\\n",
      "\\end{array}\n",
      "\\right) \\\\\n",
      "&= {\\bf X}\\ {\\bf \\theta}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we can \"vectorize\" the cost function with (note: the square is \"element by element\"):\n",
      "\n",
      "\\begin{align}\n",
      "J(\\theta) &= \\frac{1}{2m} \\sum_{i=0}^{m}\\left[\\left({\\bf X}\\ {\\bf \\theta} - {\\bf y}\\right)_i^2\\right] \\\\\n",
      "&= \\frac{1}{2m} \\left[\\left({\\bf X}\\ {\\bf \\theta} - {\\bf y}\\right)^T\\left({\\bf X}\\ {\\bf \\theta} - {\\bf y}\\right) \\right]\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1-D example\n",
      "\n",
      "Implement a 1-D linear regression with one variable to predict profits for a food truck.  You want to predict profits based on the size of the population of a city.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setup Notebook"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup notebook\n",
      "import itertools as it\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as st\n",
      "from IPython.display import display, Math, Latex\n",
      "import os\n",
      "os.chdir('%s/courses/coursera_ml008/week2' % os.getenv('DST'))\n",
      "print(os.getcwd())\n",
      "\n",
      "# plotting options\n",
      "np.set_printoptions(precision=4)\n",
      "plt.rc('font'  , size=18)\n",
      "plt.rc('figure', figsize=(10, 8))\n",
      "plt.rc('axes'  , labelsize=22)\n",
      "plt.rc('legend', fontsize=16)\n",
      "\n",
      "np.set_printoptions(precision=4)\n",
      "plt.rc('figure', figsize=(10, 8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('ex1data1.txt', names=['population', 'profit'])\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "construct the $\\bf{X}$ and $\\bf{y}$ matrices:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.insert(loc=0, column='constant', value=np.ones(df.population.shape))\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.matrix(df.as_matrix(['constant', 'population']))\n",
      "X[0:5,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = X[:,1]\n",
      "x[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.matrix(df.as_matrix(['profit']))\n",
      "y[0:5,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "plot it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x, y,'rx')\n",
      "plt.ylabel('profit in $10k')\n",
      "plt.xlabel('population in 10k')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Apply Gradient Descent\n",
      "\n",
      "First write a function for the cost function.  The linear function cost function is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost(X, y, theta):\n",
      "    m = len(y) # size of training set\n",
      "    return (1.0/(2*m)) * np.sum(np.power(X*theta - y,2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the initial step, we assign \n",
      "\n",
      "\\begin{align}\n",
      "{\\bf \\theta} = \n",
      "\\left(\n",
      "\\begin{array}{c}\n",
      "0 \\\\ \n",
      "0 \\\\ \n",
      "\\end{array}\n",
      "\\right)\n",
      "\\end{align}\n",
      "\n",
      "compute the intial value (should be 37.02 according to notes)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta = np.matrix([[0.0],[0.0]])\n",
      "theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "j0 = cost(X, y, theta)\n",
      "j0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we implement a function to apply gradient descent to find the $\\bf \\theta$ that minimized the cost fuction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_descent(X, y, theta, alpha, N):\n",
      "    m = len(y)\n",
      "    J = np.zeros(N)\n",
      "    \n",
      "    # interate\n",
      "    for n in np.arange(N):\n",
      "        # compute cost for this iteration\n",
      "        J[n] = cost(X, y, theta)\n",
      "        \n",
      "        # iterate theta\n",
      "        for j in np.arange(len(theta)):\n",
      "            x_j = np.asarray(X[:,j])\n",
      "            e   = np.asarray(X*theta - y)\n",
      "            theta[j] = theta[j] - (alpha/m)*np.sum(e*x_j) # want element by element\n",
      "            \n",
      "    return (theta, J)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apply Gradient Descent for 1500 iterations and $\\alpha = 0.01$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N     = 1500\n",
      "alpha = 0.01\n",
      "(theta, J) = gradient_descent(X, y, theta, alpha, N)\n",
      "theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the cost function versus iteration to ensure it converges:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(np.arange(N), J)\n",
      "plt.ylabel('J($\\theta$)')\n",
      "plt.xlabel('iteration')\n",
      "plt.axis([0, 1500, 4, 8])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "plot the fitted value against the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b = np.asarray(theta)[0]\n",
      "m = np.asarray(theta)[1]\n",
      "h = m*np.asarray(x) + b\n",
      "\n",
      "plt.plot(x, y, 'rx')\n",
      "plt.plot(x, h, 'b-')\n",
      "plt.plot()\n",
      "plt.ylabel('profit in $10k')\n",
      "plt.xlabel('population in 10k')\n",
      "plt.legend(['Training data', 'fit via grad descent'], loc='lower right')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Visualize $J(\\theta)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "compute $J(\\theta)$ over the parameter space of ${\\bf \\theta}$ and visualize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# grid of theta values\n",
      "theta0 = np.linspace(-10, 10, 100)\n",
      "theta1 = np.linspace(-1 ,  4, 100)\n",
      "\n",
      "# computer J for each (theta0, theta1) pair\n",
      "J = np.zeros((len(theta0), len(theta1)))\n",
      "for i, t0 in enumerate(theta0):\n",
      "    for j, t1 in enumerate(theta1):\n",
      "        t = np.matrix([[t0],[t1]])\n",
      "        J[i,j] = cost(X, y, t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import axes3d\n",
      "from matplotlib import cm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure()\n",
      "ax = fig.gca(projection='3d')\n",
      "ax.plot_surface(theta0, theta1, J)\n",
      "ax.set_xlabel('$\\\\theta_0$')\n",
      "ax.set_ylabel('$\\\\theta_1$')\n",
      "ax.set_zlabel('$J(\\\\theta)$')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.contour(theta0, theta1, J.T, levels=np.logspace(-2, 3, 20))\n",
      "plt.plot(theta[0], theta[1], 'rx', markersize=10, linewidth=2)\n",
      "plt.xlabel('$\\\\theta_0$')\n",
      "plt.ylabel('$\\\\theta_1$')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}